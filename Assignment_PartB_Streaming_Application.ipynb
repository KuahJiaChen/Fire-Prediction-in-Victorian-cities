{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part B Task 1.d Streaming Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Kuah Jia Chen <br>\n",
    "Student ID: 32286988"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, element_at, when\n",
    "from json import *\n",
    "import geohash2 as pgh\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a global variable to store topic name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'AssignmentPartB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our spark session with `#threads = #logicalCPU` and the given application name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Spark Streaming from Kafka into MongoDB for FIT3182 Assignment')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a streaming dataframe with options providing the bootstrap server(s) and topic name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\n",
    "    .option('subscribe', topic_name)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema for this dataframe to see what columns I have to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate our output stream. We cast `col('value')` and `cast col('value')` as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_stream_df = (\n",
    "    topic_stream_df\n",
    "    .select(\n",
    "       topic_stream_df.key.cast('string').alias('key'),\n",
    "       topic_stream_df.value.cast('string').alias('data')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the schema of the result dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a connection to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "# Create a MongoClient/Mongodb connection \n",
    "client = MongoClient()\n",
    "\n",
    "# We will use the database: fit3182_assignment_db\n",
    "db = client.fit3182_assignment_db\n",
    "\n",
    "# The collection name is assignment_partB\n",
    "assignment_partB = db.assignment_partB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each batch, I will process the data using the process_batch function. I will provide a brief explanation regarding the steps of processing data within the process_batch function. Please refer to the inline comments for a more details explanation.\n",
    "\n",
    "Here are the steps of processing each batch:\n",
    "\n",
    "1. pre-process the raw data by changing them to a dictionary\n",
    "2. check if there is climate data in this batch, if not, just return nothing, else continue the following steps\n",
    "3. compute the geohash for climate data using precision 3\n",
    "4. filter all the hotspot data such that only the hotspot data with the same location (i.e., same geohash with precision 3) as climate are kept\n",
    "5. if no hotspots with the same location as the climate data, just insert climate data directly to the MongoDB database\n",
    "6. otherwise, if there is at least one hotspot data with the same location as climate data, we will continue the following steps\n",
    "7. add datetime, date, pgh encoding with precision = 5 to the hotspots data\n",
    "8. group by the hotspot data based on the pgh_encoding and aggregate average of surface temperature and confidence\n",
    "9. detect the cause of the fire and insert \"natural\" or \"other\" to each hotspot data based on the air temperature and GHI\n",
    "10. delete time and pgh_encoding information for each hotspot data as they should not be part of the data, and also remove the key \n",
    "11. create the embedded model by inserting hotspots data into the climate data, which is the same as part A\n",
    "12. insert the result climate data to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df,batch_id):\n",
    "    \n",
    "    # raw data\n",
    "    raw_data = batch_df.collect()\n",
    "\n",
    "    # pre-process the raw data\n",
    "    data = []\n",
    "    for raw in raw_data:\n",
    "        current = raw.asDict()\n",
    "        current['data'] = loads(current['data']) # change it to dictionary\n",
    "        data.append(current)\n",
    "    \n",
    "    # check if there is climate data in this batch, if no, just return nothing\n",
    "    has_climate = False\n",
    "    climate = []\n",
    "    hotspots = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['key'] == 'climate':  # check the key\n",
    "            has_climate = True\n",
    "            climate.append(data[i])      # append climate data to climate list\n",
    "        else:\n",
    "            hotspots.append(data[i])     # append hotspot data to hotspots list\n",
    "            \n",
    "    if has_climate == False:             # if no climate data in this batch, just return nothing\n",
    "        return\n",
    "    \n",
    "    # compute the geohash for climate data and filter hotspots data with the same location as climate\n",
    "    climate_geohash = pgh.encode(climate[0]['data']['latitude'],climate[0]['data']['longitude'],precision=3)\n",
    "\n",
    "    filtered_hotspots = []\n",
    "    \n",
    "    for i in range(len(hotspots)):\n",
    "        \n",
    "        # find the geohash for each hotspot data\n",
    "        current_hotspots_geo_hash = pgh.encode(hotspots[i]['data']['latitude'],hotspots[i]['data']['longitude'],precision=3)\n",
    "        \n",
    "        # if it has the same geohash encoding as the climate data, add it to the filtered_hotspots list\n",
    "        if climate_geohash == current_hotspots_geo_hash:\n",
    "            filtered_hotspots.append(hotspots[i])\n",
    "            \n",
    "    # if no hotspots with the same location, just insert climate data directly to the mongodb database and return nothing\n",
    "    if len(filtered_hotspots) == 0:\n",
    "        # insert to the mongodb\n",
    "        climate[0]['data']['hotspots'] = []\n",
    "        assignment_partB.insert_one(climate[0]['data'])\n",
    "        return\n",
    "    \n",
    "    # otherwise there is at least one hotspot data with the same location as climate data\n",
    "    # hence add datetime, date, pgh encoding with precision = 5 to the hotspots data\n",
    "    for i in range(len(filtered_hotspots)):\n",
    "        \n",
    "        # add datetime for each hotspot data\n",
    "        \n",
    "        # combine the date from climate data and time from hotspot data to generate datetime information\n",
    "        current_date = datetime.strptime(climate[0]['data']['date'], '%d-%m-%Y')\n",
    "        current_time = datetime.strptime(filtered_hotspots[i]['data']['time'], '%H:%M:%S').time()\n",
    "        date_time = datetime.combine(current_date, current_time)\n",
    "        date_time = date_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        filtered_hotspots[i]['data']['datetime'] = date_time\n",
    "        \n",
    "        \n",
    "        # add date for each hotspot data by using the date from climate data\n",
    "        filtered_hotspots[i]['data']['date'] = climate[0]['data']['date']\n",
    "        #print(filtered_hotspots[i]['data']['date'])\n",
    "        \n",
    "        # add pgh encoding for each hotspot data with a precision of 5 using its latitude and longitude\n",
    "        current_latitude = filtered_hotspots[i]['data']['latitude']\n",
    "        current_longitude = filtered_hotspots[i]['data']['longitude']\n",
    "        filtered_hotspots[i]['data']['pgh_encoding'] = pgh.encode(current_latitude,current_longitude,precision=5)\n",
    "        \n",
    "        \n",
    "    # group by the hotspot data based on the pgh_encoding and aggregate average of surface temperature and confidence\n",
    "    \n",
    "    \"\"\"\n",
    "    Therefore, for each group of hotspots data with same pgh_encoding, after grouping them, the data would be:\n",
    "    \n",
    "    latitude: the value of the first hotspot data of the group\n",
    "    longitude: the value of the first hotspot data of the group\n",
    "    confidence: the average of all hotspot data with the same pgh_encoding\n",
    "    surface_temperature_celcius: the average of all hotspot data with the same pgh_encoding\n",
    "    date: the value of the first hotspot data of the group\n",
    "    datetime: the value of the first hotspot data of the group\n",
    "    pgh_encoding: will be remove after group by\n",
    "    \n",
    "    Please note that I did not check whether the hotspot is come from AQUA or TERRA as the Q4 2) in FAQ said that I am allow\n",
    "    to just find the average values for each data that are at the same location regardless of the key (i.e., AQUA, TERRA) \n",
    "    and also use the latitude, longitude, date and datetime of the first hotspot data for each group.\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the distinct pgh_encoding and store it in all_pgh_encoding\n",
    "    all_pgh_encoding = []\n",
    "    for i in range(len(filtered_hotspots)):\n",
    "        current_encoding = filtered_hotspots[i]['data']['pgh_encoding']\n",
    "        if current_encoding not in all_pgh_encoding:\n",
    "            all_pgh_encoding.append(current_encoding)\n",
    "    \n",
    "    \n",
    "    # group hotspot data with same encoding in a same sublist\n",
    "    grouped_hotspots_data = []\n",
    "    for pgh_encoding in all_pgh_encoding:\n",
    "        temp = []\n",
    "        for i in range(len(filtered_hotspots)):\n",
    "            if filtered_hotspots[i]['data']['pgh_encoding'] == pgh_encoding:\n",
    "                temp.append(filtered_hotspots[i])\n",
    "        grouped_hotspots_data.append(temp)\n",
    "        \n",
    "    \n",
    "    # for each group, find the average of confidence and surface_temperature \n",
    "    # and store it in the first document of each group\n",
    "    for group in grouped_hotspots_data:\n",
    "        sum_confidence = 0\n",
    "        sum_surface_temperature = 0\n",
    "        for i in range(len(group)):\n",
    "            sum_confidence += group[i]['data']['confidence']\n",
    "            sum_surface_temperature += group[i]['data']['surface_temperature_celcius']\n",
    "        average_confidence = sum_confidence/len(group)\n",
    "        average_surface_temperature = sum_surface_temperature/len(group)\n",
    "        \n",
    "        # store the value in the first document as eventually we only get the first document for each group as it stores\n",
    "        # all the required information\n",
    "        group[0]['data']['confidence'] = average_confidence\n",
    "        group[0]['data']['surface_temperature_celcius'] = average_surface_temperature\n",
    "        \n",
    "        \n",
    "    # filter the list such that the result list only contains the first document of each group as it stores all the \n",
    "    # required information\n",
    "    hotspots = []\n",
    "    for group in grouped_hotspots_data:\n",
    "        hotspots.append(group[0])\n",
    "    \n",
    "    \n",
    "    # detect the cause of fire and insert \"natural\" or \"other\" to each hotspot data based on the air temperature and GHI\n",
    "    if climate[0]['data']['air_temperature_celcius'] > 20 and climate[0]['data']['GHI_w/m2'] > 180:\n",
    "        cause_of_fire = \"natural\"\n",
    "    else:\n",
    "        cause_of_fire = \"other\"\n",
    "    \n",
    "    for i in range(len(hotspots)):\n",
    "        hotspots[i]['data']['cause_of_fire'] = cause_of_fire\n",
    "\n",
    "    \n",
    "    # delete time and pgh_encoding as they should not be part of the data, and also remove the key so that hotspots list\n",
    "    # only store the dictionary of the data\n",
    "    for i in range(len(hotspots)):\n",
    "        del hotspots[i]['data']['time']\n",
    "        del hotspots[i]['data']['pgh_encoding']\n",
    "        hotspots[i] = hotspots[i]['data']\n",
    "\n",
    "    \n",
    "    # create the embedded model by inserting hotspots data to the climate data, which is same as part A\n",
    "    climate_data = climate[0]['data']\n",
    "    climate_data['hotspots'] = hotspots\n",
    "\n",
    "    # insert the result climate data to mongodb\n",
    "    assignment_partB.insert_one(climate_data)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our stream writer for the MongoDB database sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_writer = (\n",
    "    output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime='10 seconds')\n",
    "    .foreachBatch(process_batch)  # batch and process data in micro-batch intervals of 10 seconds \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbb28f5a3d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_writer.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
